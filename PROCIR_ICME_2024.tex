% Template for Elsevier CRC journal article
% version 1.2 dated 17 May 2021

% This file (c) 2009-2021 Elsevier Ltd.  Modifications may be freely made,
% provided the edited file is saved under a different name

% This file contains modifications for Transportation Research Procedia

% Changes since version 1.1
% - added "procedia" option compliant with ecrc.sty version 1.2a
%   (makes the layout approximately the same as the Word CRC template)
% - added example for generating copyright line in abstract

%-----------------------------------------------------------------------------------

%% This template uses the elsarticle.cls document class and the extension package ecrc.sty
%% For full documentation on usage of elsarticle.cls, consult the documentation "elsdoc.pdf"
%% Further resources available at http://www.elsevier.com/latex

%-----------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                          %%
%% Important note on usage                                  %%
%% -----------------------                                  %%
%% This file should normally be compiled with PDFLaTeX      %%
%% Using standard LaTeX should work but may produce clashes %%
%%                                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The ’3p’ and ’times’ class options of elsarticle are used for Elsevier CRC
%% The ’procedia’ option causes ecrc to approximate to the Word template
\documentclass[5p,times,procedia]{elsarticle}
\flushbottom

%% The `ecrc’ package must be called to make the CRC functionality available
\usepackage{ecrc}
%\usepackage{amsmath}


%% The ecrc package defines commands needed for running heads and logos.
%% For running heads, you can set the journal name, the volume, the starting page and the authors

%% set the volume if you know. Otherwise `00’
\volume{00}

%% set the starting page if not 1
\firstpage{1}

%% Give the name of the journal
\journalname{Procedia CIRP}

%% Give the author list to appear in the running head
%% Example \runauth{C.V. Radhakrishnan et al.}
\runauth{B. Bertschinger et al.}

%% The choice of journal logo is determined by the \jid and \jnltitlelogo commands.
%% A user-supplied logo with the name <\jid>logo.pdf will be inserted if present.
%% e.g. if \jid{yspmi} the system will look for a file yspmilogo.pdf
%% Otherwise the content of \jnltitlelogo will be set between horizontal lines as a default logo

%% Give the abbreviation of the Journal.
\jid{trpro}

%% Give a short journal name for the dummy logo (if needed)
%\jnltitlelogo{Transportation Research}

%% Hereafter the template follows `elsarticle’.
%% For more details see the existing template files elsarticle-template-harv.tex and elsarticle-template-num.tex.

%% Elsevier CRC generally uses a numbered reference style
%% For this, the conventions of elsarticle-template-num.tex should be followed (included below)
%% If using BibTeX, use the style file elsarticle-num.bst

%% End of ecrc-specific commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The amssymb package provides various useful mathematical symbols

\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%\biboptions{authoryear}

% \biboptions{}

% if you have landscape tables
\usepackage[figuresright]{rotating}
%\usepackage{harvard}
% put your own definitions here:x
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

% add words to TeX’s hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

% declarations for front matter

\usepackage[bookmarks=false]{hyperref}
\hypersetup{colorlinks,
linkcolor=blue,
citecolor=blue,
urlcolor=blue}

\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{subcaption}
%\usepackage{kbordermatrix}
\usepackage{tikz}
\usepackage{relsize}
\usepackage{lipsum}

\AtBeginDocument{%
\addtolength\abovedisplayskip{-1.0\baselineskip}%
\addtolength\belowdisplayskip{-1.0\baselineskip}%
\addtolength\abovedisplayshortskip{-1.5\baselineskip}%
\addtolength\belowdisplayshortskip{-1.0\baselineskip}%
%\setlength{\textfloatsep}{1pt }
\setlength{\abovecaptionskip}{0.5\baselineskip} 
\setlength{\belowcaptionskip}{-0.5\baselineskip} 
}

% figures
\usepackage{tikz}
\usepackage{tikzscale}
\usepackage{pgf}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=newest}

\usetikzlibrary{calc,
	%	external,
	pgfplots.units,
	%	pgfplots.external,
	pgfplots.groupplots,
	arrows,
	intersections,
	patterns,
	positioning,
	shapes,
	plotmarks,
	decorations.pathmorphing,
	decorations.markings
}

\begin{document}
\begin{frontmatter}
	
	%% Title, authors and addresses
	
	%% use the tnoteref command within \title for footnotes;
	%% use the tnotetext command for the associated footnote;
	%% use the fnref command within \author or \address for footnotes;
	%% use the fntext command for the associated footnote;
	%% use the corref command within \author for corresponding author footnotes;
	%% use the cortext command for the associated footnote;
	%% use the ead command for the email address,
	%% and the form \ead[url] for the home page:
	%%
	%% \title{Title\tnoteref{label1}}
	%% \tnotetext[label1]{}
	%% \author{Name\corref{cor1}\fnref{label2}}
	%% \ead{email address}
	%% \ead[url]{home page}
	%% \fntext[label2]{}
	%% \cortext[cor1]{}
	%% \address{Address\fnref{label3}}
	%% \fntext[label3]{}
	
	\dochead{18th CIRP Conference on Intelligent Computation in Manufacturing Engineering}%
	
	\title{A new Software Driven External Sensor System for Industrial Robots}
	
	%% use optional labels to link authors explicitly to addresses:
	%% \author[label1,label2]{<author name>}
	%% \address[label1]{<address>}
	%% \address[label2]{<address>}
	
	\author[a]{Bernd Bertschinger\corref{*}}
	\author[b]{Kathrin Hoffmann}
	\author[c]{Jan Baumgärtner}
	\author[b]{Gajanan Kanagalingam}
	\author[c]{Jürgen Fleischer}
	\author[b]{Oliver Sawodny}
	\author[a]{Stephan Reichelt}
	%\ead{author@institute.xxx}
	
	\address[a]{Institute of Applied Optics, University of Stuttgart - ITO, Pfaffenwaldring 9, 70569 Stuttgart, Germany}
	\address[b]{Institute for System Dynamics, University of Stuttgart - ISYS, Waldburgstr. 17/19, 70563 Stuttgart, Germany}
	\address[c]{Institute of Production Science, Karlsruhe Institute of Technology - WBK, Kaiserstraße 12, 76131 Karlsruhe, Germany}
	
	\aucores{* Bernd Bertschinger, Tel.: +49-711-685-69892. {\it E-mail address:} bernd.bertschinger@ito.uni-stuttgart.de}
	
	\begin{abstract}
		%% Text of abstract
		For decades, laser tracker and working station have been the state of the art to measure externally the position disturbances in robotic systems. High system costs limit their usage for control systems in common production machines. We present details for an alternative software-driven approach. Hereby, we combine a new self-referencing, high-precision photogrammetry sensor system with a software for camera placement layout and trajectory optimization. Furthermore, we outline the integration in a closed loop control system and corresponding strategies.
	\end{abstract}
	
	\begin{keyword}
		Type your keywords here, separated by semicolons ;
		
		%% keywords here, in the form: keyword \sep keyword
		
		%% PACS codes here, in the form: \PACS code \sep code
		
		%% MSC codes here, in the form: \MSC code \sep code
		%% or \MSC[2008] code \sep code (2000 is the default)
		
	\end{keyword}
	%\cortext[cor1]{Corresponding author. Tel.: +0-000-000-0000 ; fax: +0-000-000-0000.}
	
\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text

\section{Introduction}
%
%motivation: industrial robots instead of specialized machinery, adaptable to various tasks in manufacturing, low cost \\
%limiting factor: accuracy of robot
%
%approach: software-defined planning of the robot configuration, path and trajectory
%
%result: robot motion which is the optimal solution for production precision
%
%setup: (close to?) real-time capable optical measurement system, optical markers attached to robot endeffector
%
%Relevant Errors-Model for Process Perception - Metrological Error \\
%Relevant Errors-Model for Process Planing - Dynamic Error \\
%
%System Setup
%
Social and ecological changes, as well as the need for
highly individualized and diversified product ranges
motivate not only to increase the efficiency of the means
of production, but also their flexibility of application.
This is addressed by the concept of Wertstromkinematik \cite{Muehlbeier2020}.
Herein the factory is not a static setup to produce
one set of goods, but can be dynamically reconfigured to be a generalized production floor.\\
Therefore automated production needs not only to
be more productive, but also as flexible and cost-efficient as possible. This need for flexibility in turn leads to an increase of precision, which can be used to either facilitate the production of high-precision components, for human-robot and robot-robot collaboration, as well as a trade-off for higher process speed.\\
By the current state of technology this is a dilemma, because state of the art detection system use internal sensor, which require an exceeding level of mechanical stiffness, which in turn leads to high
acquisition costs. An alternative to internal sensor system are external sensors, such laser trackers and tracking stations \cite{Moeller17, Yang17}. The costs of these systems make them unsuitable for wide-spread production applications, but are rather used as means for calibration.\\

\subsection{Scope of this Paper}
%
Instead a holistic approach is needed, which combines in a software driven production process the placement and configuration of the sensor system with the trajectory generation in consideration of the kinematic control properties.
%
%\lipsum[1-1]
%
%\vspace*{2pt}
%\renewcommand{\labelenumii}{\theenumii}
%\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
%\vspace*{8pt}
%\setlist{nolistsep}
%
%
\section{Process Perception}
%
Recent research indicates that multipoint photogrammetry might be a solution to realize cost-efficient high-resolution optical sensors \cite{Hartlieb_2021}.
The schematics of this measuring method is shown in Fig.~\ref{fig:MeasSys_Errors}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\columnwidth]{graphics/error_sources.png}
	\caption{Error sources in a robotic manufacturing system, which can be compensated using an external multipoint  sensor are marked in green.}
	\label{fig:MeasSys_Errors}
\end{figure}
In this method, as can be seen in Fig.~\ref{fig:error_sources}, a Diffractive Optical Element (DOE) is used in succession of an imaging lens to create multiple linearly independent optical copies of an active marker.
It has been shown \cite{Hartlieb_2021} that the variance of the mean positional detection error of these points $\sigma_{x’y’}^2$ detected on the imaging sensor is proportional to the number of copies $n_{pt}$ created by the DOE.\\
However, this increase in planar signal detection is in itself not sufficient for high precision 3D localization. One or more additional sensors is needed to triangulate the 2D point positions on the imaging sensors to it’s point of origin in the object space. \\
For a pinhole camera system the relation between object and imaging point is determined by the camera Matrix $\mathbf{C}$, which is composed of the intrinsic $\mathbf{K}$ and  and extrinsic camera parameters $[\mathbf{R}, \mathbf{t}]$, which comprise information about the spatial orientation, as well as the information about focal width and
image scale factor. This leads to the well established collinear equation \cite{Luhmann2003}, whereby $l_n$ is the collinear factor, which describes the line of sight between the object and image point.
%
\begin{align}
	\mathbf{C} = \mathbf{K}
	\begin{bmatrix}
		\mathbf{R} & \mathbf{t} \\
		0 & 1 \\
	\end{bmatrix} \\
	l_{n}
	\begin{bmatrix}
		x_n’ \\
		y_n’ \\
		0
	\end{bmatrix}
	= \mathbf{C}
	\begin{bmatrix}
		x \\
		y \\
		z \\
		0
	\end{bmatrix}
\end{align}
Subsequently the correspondance between object position an image points, known as triangulation, can be defined for multiple cameras according to:
%
%https://cookierobotics.com/007/
\begin{equation}
	\label{eqn:triangulation}
	\begin{bmatrix}
		x_{1}’ \mathbf{C}_{3,1}^{\top} - \mathbf{C}_{1,1}^{\top}\\
		y_{1}’ \mathbf{C}_{3,1}^{\top} - \mathbf{C}_{2,1}^{\top}\\
		x_{2}’ \mathbf{C}_{3,2}^{\top} - \mathbf{C}_{1,2}^{\top}\\
		y_{2}’ \mathbf{C}_{3,2}^{\top} - \mathbf{C}_{2,2}^{\top}\\
		\vdots \\
		x_{n}’ \mathbf{C}_{3,n}^{\top} - \mathbf{C}_{1,n}^{\top}\\
		y_{n}’ \mathbf{C}_{3,n}^{\top} - \mathbf{C}_{2,n}^{\top}\\
	\end{bmatrix}
	\begin{bmatrix}
		x \\
		y \\
		z \\
		1
	\end{bmatrix}
	=
	\begin{bmatrix}
		0 \\
		\vdots \\
		0
	\end{bmatrix},
\end{equation}
%
where $C_{m,n}$ corresponds to the $m$-th row of the $n$-th camera matrix.
In case the extrinsic and intrinsic camera parameters are fully known, the equation system can be solved by least-square techniques \cite{Ahn2004}.
%
Whereby $C_{m,n}$ corresponds to the m-th row of the n-th camera Matrix.
In case the extrinsic and intrinsic camera parameters are fully known, the system can be solved by the least-square techniques \cite{Ahn2004}.
%
It is important to note that this relationship can also be described as by the notation for projective reconstruction \cite{Hartley2018}:
\begin{equation}
	\label{eqn:ProjectiveReconstruction}
	x’^{\top}\mathbf{F}x
\end{equation}
%
Here the fundamental matrix $\mathbf{F}$ is a $3\times 3$ tensor, which scales in rank according to the number of cameras in the system. Due to the increasing complexity and computing effort, this method is usually constraint to three cameras systems.\\

Instead of a Tensor representation the triangulation relation (\ref{eqn:triangulation}) is stated in implicit form
%
\begin{equation}
	\label{eqn:ImplicitFrom}
	\begin{aligned}
		& g(\mathbf{Q},\mathbf{M}) \\
		& \mathbf{Q} = [\mathbf{P},\mathbf{C}]^{\top}.
	\end{aligned}
\end{equation}
%
The vector of the input Quantities $\mathbf{Q} = \left[q_1,\dots, q_{n}\right]^{\top}$ is composed of the vector of image points $\mathbf{P} = [x’_1,y’_1, \dots ,x’_n,y’_n]^{\top}$ and the vector of camera parameters, $\mathbf{C} = \left[ \mathbf{C}_1 , \dots , \mathbf{C}_n \right]^{\top}$, which in turn define the object point coordinates $\mathbf{M} =  [x,y,z]^{\top}$ in Cartesian form. \\
The uncertainty of the output $\mathbf{M}$ can be approximated by the 1st order Taylor series with regards to input parameters:
%
\begin{equation}
	\sigma^2 = \sum_{i=1}^{n}\sum_{j=1}^{n} \left(\frac{\delta g}{\delta q_i}\right) \left(\frac{\delta g}{\delta q_j}\right) \mathrm{cov}(q_i, q_j) 
\end{equation}
where $\mathrm{cov}(q_i, q_j) $ is the covariance between two input parameters. More commonly it is known in the form~\cite{Cox2006}  
\begin{equation}
	\sigma^2 = \mathbf{J_{Q}}\mathbf{\Lambda_{Q}}\mathbf{J_{Q}}^{\top}.
\end{equation}
$\mathbf{J_{Q}}$ is known as design or Jacobi matrix of the partial derivatives of $g(\mathbf{Q},\mathbf{M}$ with respect to the input quantities $\mathbf{Q}$, and $\mathbf{\Lambda_Q}$ is the covariance matrix of said input values. \\
The uncertainty of the input parameters is directly tied to the uncertainty of the output parameters
\begin{equation}
	\mathbf{J_{M}}\mathbf{\Lambda_{M}}\mathbf{J_{M}}^{\top} = \mathbf{J_{Q}}\mathbf{\Lambda_{Q}}\mathbf{J_{Q}}^{\top}
\end{equation}
Therein, $\mathbf{J_{M}}$ is the Jacobi matrix of the partial derivatives input quantities $\mathbf{M}$ and $\mathbf{\Lambda_{M}}$ is the covariance matrix with respect to the output quantities $\mathbf{M}$.
The matrix $\mathbf{\Lambda_{M}}$ is computed as
\begin{equation}
	\mathbf{\Lambda_{M}} = \mathbf{J_{M}^{\Delta}} \left( \mathbf{J_{Q}}\mathbf{\Lambda_{Q}}\mathbf{J_{Q}}^{\top}\right) \left(\mathbf{J_{M}^{\Delta}}\right)^{\top}
\end{equation}
with $ \mathbf{J_{M}^{\Delta}} = \left( \mathbf{J_{M}^{\top}} \mathbf{J_{M}^{}} \right)^{-1}\mathbf{J_{M}^{\top}}$ being the Moore-Penrose pseudo inverse matrix of $\mathbf{J_M}$.
%
\subsection{Metrological Error}
\label{error_estimate}
%
The design matrices for the input $\mathbf{J}_{Q}$ and output quantities $\mathbf{J}_{M}$ can be established by a various calibration processes \cite{Hartley2018}. Usually a series of known markers is placed in the object space \cite{Luhmann2003}. In case of arbitrary camera positions the minimum number of marker needed corresponds to
\begin{equation}
	\label{eqn:NumCalibPoints}
	\begin{aligned}
		& 	u = u_n \cdot n_{pictures} + u_p \cdot n_{points} + u_k \cdot n_{cameras} \\
		& \text{with: } u_n = 6\text{, } u_p = 3\text{, } u_k = 6 
	\end{aligned}
\end{equation}

Whereby $u_n$,$u_p$,$u_p$ is the minimum number of parameters
\textcolor{red}{Variablen einführen}

It becomes evident, that an increase in the number of cameras $n_{\text{cameras}}$ leads to increasing requirements for the means of calibration (\ref{eqn:NumCalibPoints}) and computational complexity, signified by the increasing rank of the fundamental matrix (\ref{eqn:ProjectiveReconstruction}). Conversely it becomes difficult to determine the metrological error of such as system.\\
%
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{graphics/MixedErrorCuttingGeometry.eps}
	\caption{Top view on a photogrammetric sensor in tricfocal configuration under simplified converging geometric constraints, whereby the optical axes $OA$ of three cameras systems intersect in one point.		
	It can also be regarded as as composition of $n_{sub}=3$ stereoscopic subsystems, whereby the measurement error of each subsystem corresponds in shape to an ellipsoid $\mathbf{E}$.}
	\label{fig:sub_sensors}
\end{figure}
%
Hence an alternative approach is needed. As it is depicted in  Fig.~\ref{fig:sub_sensors} a photogrammetric sensor can also be described as a combination of multiple independent stereoscopic sub-sensors.
%
\begin{equation}
	\label{eqn:CovarianceMatrix}
	n_{sub} = \sum_{n=1}^{n_{cam}}n-1
\end{equation}
%
For these subsensors the metrological error can be described as an ellipsoid $\mathbf{E}$ \cite{Luhmann2003}, which can be related to spectral decomposition of the covariance of the output quantities $\mathbf{\Lambda_{M}}$.
%
\begin{equation}
	\mathbf{\Lambda_{M}} =
	\begin{bmatrix}
		\mathbf{S}_1^{} & \mathbf{S}_2^{} & \mathbf{S}_3^{}
	\end{bmatrix}^{\top}
	\begin{bmatrix}
		\lambda_1^{} & 0 \\
		0 & \lambda_2^{} &  0 \\
		0 & 0 &  \lambda_3^{}
	\end{bmatrix}
	\begin{bmatrix}
		\mathbf{S}_1^{} \\
		\mathbf{S}_2^{} \\
		\mathbf{S}_3^{}
	\end{bmatrix}.
\end{equation}
%
The normalized Eigenvectors $\mathbf{\hat{S}}_i = \mathbf{S}_i / |\mathbf{S}|$ correspond to the direction of the semi-axis of the ellipsoid and the square root of product of the Eigenvalues $\lambda_i$ and the quantile of the $\chi^2_{3,1-\alpha} $ distribution with regards to the probability of safety $1-\alpha$ \cite{Pelzer1995}.
%
%https://cookierobotics.com/007/
%https://en.wikipedia.org/wiki/Weighted_arithmetic_mean
\begin{equation}
	\mathbf{E} =
	\begin{bmatrix}
		e_{11}^{} & e_{12}^{} & e_{13}^{} \\
		e_{21}^{} & e_{22}^{} & e_{23}^{} \\
		e_{31}^{} & e_{32}^{} & e_{33}^{}
	\end{bmatrix}
	=
	\sqrt{ \chi^2_{3,1-\alpha}}
	\begin{bmatrix}
		\mathbf{\hat{S}}_1^{} & \mathbf{\hat{S}}_2^{} & \mathbf{\hat{S}}_3^{}
	\end{bmatrix}
	\begin{bmatrix}
		\sqrt{\lambda_1^{}} \\
		\sqrt{\lambda_2^{}} \\
		\sqrt{\lambda_3^{}}
	\end{bmatrix}
\end{equation}
\textcolor{red}{verstehe den Satz nicht}
Hence the metrological error along the unit vectors of the reference coordinate system equals
\begin{equation}
	\sigma_j = \sum_{j=1}^{3} \left( e_{ij} + e_{ij} + e_{ij}\right)
\end{equation}
%
Subsequently the metrological error for each subsystem $\sigma_{j,n}$ can be used to combine and improve the related outcomes $\mathbf{M}_n$ by calculating the weighted arithmetic mean \cite{Price1972}
\begin{equation}
	\mathbf{\bar{M}}_{j} = \frac{\sum_{n=1}^{n_{sub}} \left( \mathbf{M}_{n,j} \cdot \sigma_{n,j}^{-2} \right)}{\sum_{n=1}^{n_{sub}} \sigma_{n,j}^{-2}}
\end{equation}
The resulting error corresponds to
\begin{equation}
	\mathbf{\bar{\sigma}}_{j} = \sqrt{ \frac{1}{\sum_{n=1}^{n_{sub}} \sigma_{n,j}^{-2}} }.
	\label{eqn:sum_noise}
\end{equation}
%
%
\subsection{Optimized Perception}
It has been remarked in the literature~\cite{Di_Leo_2011} that a covariance exists between the input quantities of a stereoscopic system and it is related to a combination of marker and the image processing algorithm used for detection.
Only recently, for the case of a simplified stereoscopic sensor under converging constraints with passive markers, simulation results have experimentally been verified.
Liu et al.~\cite{Liu_2021} showed that the optimum camera orientation angle $\beta_{mn}$ is in the scope of $60^{\circ} -\, 80^{\circ}$, not $30^{\circ} \text{-}\, 50^{\circ}$ \cite{Yang2018,Fooladgar2013,Sankowski2017}. The input quantities for such a system are depicted in Fig.~\ref{fig:sub_sensors} and read
\begin{equation*}
	\mathbf{Q}= \left[x’_m, y’_m, x’_n, y’_n, \beta_{mn}, \beta_{nm}, B_{nm}, f_{n},f_{m}\right]^{\top}
\end{equation*}
In this case $x’, y’$ is the pixel position of the marker on the sensor plane, $\beta$ is the aforementioned camera orientation angle, $B$ is the distance between the pinhole position and $f$ is the focal length. 
%\\The covariance matrix becomes:
%\begin{equation}
%	\label{eqn:CovarianceMatrix}
%	\mathbf{\Lambda_{Q}} = 
%	\begin{bmatrix}
%		\sigma_{\delta_1}^2  & 0 & 0 & 0 & \sigma_{(\delta_1,\delta_5)}^2 & 0 & 0 & 0 & 0 \\
%		0  & \sigma_{\delta_2}^2 & 0 & 0 & \sigma_{(\delta_2,\delta_5)}^2 & 0 & 0 & 0 & 0 \\
%		0 & 0 & \sigma_{\delta_3}^2 & 0 & 0 & \sigma_{(\delta_3,\delta_6)}^2 & 0 & 0 & 0 \\
%		0 & 0 & 0 & \sigma_{\delta_4}^2 & 0 & \sigma_{(\delta_4,\delta_6)}^2 & 0 & 0 & 0 \\
%		\sigma_{(\delta_5,\delta_1)}^2 & \sigma_{(\delta_5,\delta_2)}^2 & 0 & 0 & \sigma_{\delta_5}^2 & 0 & 0 & 0 & 0 \\
%		0 & 0 & \sigma_{(\delta_6,\delta_3)}^2 & \sigma_{(\delta_6,\delta_4)}^2 & 0 & \sigma_{\delta_6}^2 & 0 & 0 & 0 \\
%		0 & 0 & 0 & 0 & 0 & 0 & \sigma_{\delta_7}^2 & 0 & 0 \\
%		0 & 0 & 0 & 0 & 0 & 0 & 0 & \sigma_{\delta_8}^2 & 0 \\
%		0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \sigma_{\delta_9}^2
%	\end{bmatrix}
%\end{equation}
The off-diagonal elements of the resulting covariance matrix $\mathbf{\Lambda_{Q}}$ signify high degrees of cross-correlation between the camera orientation angle $\beta$ and the point position $x,y$ on the detector. This is related to the decreasing visible surface of the passive markers, which leads to an increase of the effective noise per area.

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.9\linewidth]{graphics/ScatteringProfile_DiffuseFibreTip.eps}
	\caption{Radiation characteristic of an LED and diffusor fiber tip and the corresponding frame of reference.}
	\label{fig:scat_profile}
\end{figure}
%
Since multipoint detection \cite{Haist2015} uses sub-resolution active markers instead of passive ones, it cannot be assumed that the cross-correlation factors are similar. Since here the so called signal-to-noise Ratio (SNR) primarily depends on intensity of the spots on the sensor plane. \\
This signal strength is on the one hand related to the distance between sensor and marker \cite{dumbleton1955}, on the other hand it depends on the spatial orientation of the marker in regards to the receiving optic. As depicted in Fig.~\ref{fig:scat_profile}
the radiation characteristic of a LED or a diffusor fiber tip \cite{Pan1994} is for most angular orientations non-isotropic.\\
%
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.95\columnwidth]{graphics/OpticalSimulation.eps}
	\caption{Simulation procedure for multipoint detection based on a Zemax Black-Box-Model (BBM) and a combination of geometric and Fresnel propagation.}
	\label{fig:opto-sim}
\end{figure}

On this basis an optical simulation procedure was created, as can be seen in Fig.~\ref{fig:opto-sim}, to extended previous works \cite{Liu_2021,Di_Leo_2011} about the covariance-analysis for stereoscopic sensors.\\
%
In this case the extrinsic camera parameters and the marker coordinates in space and orientation are used to derive
the input vector for the simulation, which comprises information about the normalized intensity distribution $\bar{I}$, the distance to marker $z$ and the field angles $\omega$ and $\theta$. A Zemax Black Box Model of the lens is used to compute the complex wavefront distribution at the DOE-Position $\mathbf{U}_m$, which is combined with the DOE Phase Distribution and $\mathbf{U}_{DOE}$ a spherical Wavefront $\mathbf{U}_{S}$ to accommodate for the distance of the DOE to the imager. This Wavefront is propagated onto the sensor by the Fresnel method \cite{Goodman2005}. The fixed $\mathbf{I}_{DNSU/PRNU}$ and random $\mathbf{I}_{DCN/SHOT}$ noise patterns are superimposed onto the derived sensor intensity distribution $\mathbf{I}_m$ and the point coordinates $\mathbf{P}_{x’,y’}$ are derived via multipoint detection.
Finally the point position is varied by an estimate of the atmospheric disturbance.\\
%
As can be seen Eqn.~\ref{eqn:sum_noise}, one can state that more cameras lead to a smaller metrological error.
The restriction, however is, that they need to be placed such that they can perceive the target at all -- only then more cameras lead to an improved perception.
Therefore, a strategy to place the cameras is presented as a part of the process strategies in the next section.
%
\section{Process Strategies}
Besides the aforementioned placement of the cameras, incorporating their measurements also opens up new possibilities for a software-defined motion planning of the robot.
To find a compromise between accuracy and speed, the motion planning problem is divided into path planning and path trajectory generation, which is a common approach in the literature~\cite{Choset05}.
In path planning, the path of each of the robots' joint angles is computed, providing the highest possible repeatability.
In trajectory generation, the previously computed path is indexed in time so that the robot can follow the given paths as quickly as possible such that the process speed is maximized, taking into account dynamic constraints of the robot (e.g. actuator dynamics).
These steps are outlined in this section.
% Kinematic and dynamic strategies need to be separated. 
% In the field of robot trajectory generation, this has lead to the path speed decomposition framework~\cite{Choset05}, which is well known in the literature. 
% Therein, first a path is generated kinematically, and second time-indexed considering the robot dynamics. 
This also allows for a separate consideration of kinematic and dynamic errors of the robot system. 
Once the configuration of the robot is known from the motion planning steps, also the placement of the cameras for the process perception can be optimized as also described in this section. 
%
%
\subsection{Hardware implementation}
%
\textcolor{red}{hier kürzen \\
setup mit Trifocal-Sensor according to section \ref{error_estimate}\\
- Calibration exceeds scope of this paper, various techniques exist \cite{Hartley2018}.
- CanBus
- optional: robot axis used for referencing the world coordinate system
}
%
Kameras, Setup beschreiben (mehr als 2 Kameras wg. Abschattung, ergibt sich aus zuvor beschriebener Theorie)
%
%
of control system: robot w/ external sensor
calibration of camera system: interface ITO / ISYS
synchronized measurements of the camera, joint angles (encoders), joint torques 
interface through which the reference values for the joint space position, velocity and acceleration are directly communicated to the built-in robot trajectory tracking controller
%
In the present work, pneumatic robots are applied. 
%
Their drives are direct drives, which has the advantage that
%
%
The disadvantage, however, is that 
pneumatic robot - direct drives, drive dynamics 
%
If the robot endeffector is obscured and the optical measurement system cannot detect its pose (position and orientation), the pose of the endeffector is estimated using a Kalman filter with a constant velocity model. This assumption is reasonable in this case, as the measurement rate of the measurement system is significantly higher than the change in the pose of the endeffector.
However, it is advisable to place the cameras such that the perception of the process is optimized, as described later.
%
%
\subsection{Kinematic strategies}
Some kinematic strategies for increasing the accuracy of industrial robots are already well-established.
Purely geometric errors in a robot can stem from the sources length deviations of links, axis misalignment and zero-position offsets of the encoders.
There are established methods for identifying and compensating these parameters in an offline calibration technique~\cite{Wiest01}.
To this end, the endeffector pose is measured with an external measurement statically at certain points in the workspace and a certain number of times, and the geometric error parameters are determined in an optimization.
This offline calibration is usually performed at certain points, because of which it does not generalize in the entire workspace and is not applicable in the industrial robot application itself, but provides a first enhanced kinematic model of the robot.
From a kinematic point of view, if the manufacturing tolerances of the robot are known, incorporating them in the forward kinematics yields an error measure in task space. Such errors are usually much larger than the measurement accuracy of the presented measurement system.
%
\textcolor{red}{deformation}
It also captures the static deformation of the robot structure, which cannot be sep
different under different loads 
%
\subsection{Path Planning}\label{subsec:PathPlanning}
There are several error sources impeding the accuracy of a robotic manufacturing system.
An overview of these sources can be seen in Fig.~\ref{fig:error_sources}.
Most of these errors can be captured with the present measurement system and from that be compensated.
%
This leaves the finite precision of the robot itself, whose contribution can not be compensated but only mitigated.
Previous works such as \cite{previous_work} have shown that the optimal repeatability of a path is dependent on the workpiece placement.
This means that we can use the workpiece holding robot to reposition the workpiece in such a way that the optimal repeatability is achieved.
It might be tempting to try to find a continuous trajectory of this second robot to minimize the error.
However the second robot also suffers under finite joint precision, while moving these will introduce additional errors.
It is therefore better if the workpiece holding robot moves to a fixed position before the second robot starts moving.
This leads to a two step process similar to the one described in \cite{stroke_division}.
Here the authors propose a 4 step process to path planning:
\begin{enumerate}
	\item Cut path into multiple segments
	\item Downsample each segment
	\item Optimize the pose of each subpath and compute the joint path
	\item Apply the new pose to the original segments
\end{enumerate}
The division of the path into multiple segments was performed by identifying turning points using path simplification algorithms \cite{stroke_division}.
However instead of using the pose optimization algorithm described in \cite{stroke_division} we use the problem formulation of \cite{previous_work} since it allows us to integrate more constraints which are later used for the joint trajectory optimization.
% Hier ein Experiment durchführen mit den FESTO Robotern und zeigen wie sich die Genauigkeit gegen die Anzahl der Segmente verhält.
% Hier kann man dann als nächstes die Genauigkeit gegen die Taktzeit auftragen wenn man die Anzahl der Segmente variiert. Hier wird sich Pareto Kurve ergeben.
%
% The optimal Repeatability is dependent on the optimized kinematic parameters.
%
%
\subsection{Dynamic Strategies:}\label{subsec:dynError} Ongoing Research \\
literature on capturing dynamic errors:
%
here: present robot’s drives are direct drives, therefore no backlash in gearing, joint angle tracking error directly measured, 
enables dynamic error model
alternative: secondary encoders~\cite{Mesmer22}
%
closed loop control of the robot joints: tracking errors occur, but are captured in joint angle measurement
%
dynamic error model
%
data from measurement: same path executed at different speeds - dynamic error model is able to predict dynamic error of other velocity profile,
%
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.95\columnwidth]{graphics/DynError.eps}
	%\input{graphics/GP_eOf_dqDes_ddqDes_trainedOnFastestAndSlowest.tex}
	\caption{Dynamic error in angle of one exemplary joint. Fit with dynamic Gaussian process error model considering the desired angular velocity and acceleration as features.}
	\label{fig:error_sources}
\end{figure}
%
low velocity: stick-slip from friction affects accuracy,\\
high velocity: limited bandwidth of the controller has the effect that tracking errors cannot be compensated well, also affects accuracy \\
therefore prefer medium velocity
%
allows for trade-off between process speed and accuracy
%
%
%
next step: to incorporate in optimization
%
\subsection{Trajectory Generation}
After the path has been determined in section~\ref{subsec:PathPlanning} under the premise of optimal repeatability, in this section a trajectory is generated based on the given path under the premise of maximizing the robot’s travel speed and thus the process speed.
To generate the trajectory, the already determined path of all joint angles is parameterized by a path parameter, so that the paths of the individual joints of the robot can be described by this parameter, whereby the individual paths are implicitly synchronized.
Since the robot applied in this work has the mathematical property of differential flatness~\cite{Hoffmann23}, the state of the robot can be represented with the help of the given trajectory of the individual joint angles and the path parameter and its time derivatives.
In order to generate the trajectory, it is now sufficient to determine the time derivatives of the path parameters by means of an optimal control problem in such a way that the travel time is minimized, taking into account the dynamic, state-dependent constraints of the robot. 
For a more detailed description of trajectory generation, please refer to~\cite{Kanagalingam24}.
%
This existing formulation allows for extending the optimization problem from purely travel time and/or energy to also incorporating accuracy.
To this end, the data-driven error model from section~\ref{subsec:dynError} can be added to the optimal control problem.
To quantify the errors, the mean value function of the Gaussian process is added, while the covariance serves to quantify the certainty of the model, which can additionally be included in the cost functional.
Therefore, future work will include optimization-based trajectory generation with this data-driven component added to the optimal control problem.
%
\subsection{Camera Placement}
With the robot configuration being known from the previous steps, the perception of the process can be optimized as described in the following.
After planning the path the two robots are bound to perform complex movements that might obscure some makers from the camera.
It might even be the case that all markers are visible but that they are in regions where the camera system has a low measurement accuracy.
To mitigate both problems we propose a software system that can optimize the placement of the cameras as needed.
This system is largely based on the work of \cite{camera_placement} and follows a two step optimization approach.
However instead of only considering the condition of the triangulation equation as well as visibility we use the full error model described in section \ref{error_estimate}.
This allows use to way the benefits of repositioning against the additional effort needed to do reposition and recalibrate the system.
%
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.95\columnwidth]{graphics/fov_sv_conflict.png}
	\caption{The conflict between the field of view angle and the smallest singular value. Left: The smallest singular values of the system replicated from \cite{camera_placement}. Right: The largest angle of a marker to the optical axis.
		Comparing both one can see that lower singular values correlate with lower maximum field of view (FOV) angles.}
	\label{fig:fov_sv_conflict}
\end{figure}
%
It also has practical advantages.
Replicating the results of \cite{camera_placement} where the smallest singular value was used as a measure of the quality of the camera placement we can additionally plot the largest angle to the optical axis.
Here we see that the system always tries to find a trade off between minimizing the field of view angle while trying to maximize the smallest singular value.\\
This is shown in Fig.~\ref{fig:fov_sv_conflict}.
In a unified error description this problem does not exist and the system can be optimized for the best possible accuracy.
%
%
\section{Outlook}
experimental validation of measurement system
- Outlook, Planing Invertation
- Tradeoff
%
%
\section*{Acknowledgements}
The authors would like to thank the Ministry of Science, Research and Arts of the Federal State of Baden-Württemberg for the financial support of the projects within the InnovationsCampus Future Mobility (ICM).
%
\bibliographystyle{elsarticle-harv}
\bibliography{sources}
%
\clearpage\onecolumn
%
\normalMode
%
\end{document}

%%
%% End of file.
